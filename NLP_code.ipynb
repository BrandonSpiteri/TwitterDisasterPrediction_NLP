{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "circular-royal",
   "metadata": {},
   "source": [
    "## Twitter Classification\n",
    "Applying Natural Language Processing to classify if a tweet is about a real disaster or not\n",
    "\n",
    "Kindly find the required Glove Embedding text file (glove.6B.100d.txt) from:\n",
    "https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "\n",
    "Created by: Brandon Spiteri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-reviewer",
   "metadata": {},
   "source": [
    "### Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "limiting-safety",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import re\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import naive_bayes\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#seed keras model \n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(2)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import csv \n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "  \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, mean_squared_error, accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-duration",
   "metadata": {},
   "source": [
    "### Tweet preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "liberal-official",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(filename,output = False):\n",
    "    '''\n",
    "    filename - csv file to process, clean and tokenise\n",
    "    Preprocess function reads the csv file, extracts the tweet id, keyword, sentiment and message. \n",
    "    Regex functions are applied to clean the tweets.\n",
    "    The content is tokenised.\n",
    "    '''\n",
    "    clean_content_tokenized = []\n",
    "    tweet_sentiment =[]\n",
    "    tweet_id_list =[]\n",
    "    documents = []\n",
    "    tweet_keyword_list=[]\n",
    "\n",
    "    \n",
    "    with open(filename,'r', encoding=\"utf8\") as f:\n",
    "        next(f)\n",
    "\n",
    "        tweet = csv.reader(f)\n",
    "        for row in tweet: \n",
    "\n",
    "            # store 1st col of tweet as tweet_id\n",
    "            tweet_id = int(row [0])\n",
    "            tweet_id_list.append(tweet_id)\n",
    "\n",
    "            #store 2nd col as keyword\n",
    "            tweet_keyword = row[1]\n",
    "            tweet_keyword_list.append(tweet_keyword) \n",
    "\n",
    "            if output ==True:\n",
    "                # second col as sentiment (pos/neg/neutral)\n",
    "                sentiment = int(row[4])\n",
    "                tweet_sentiment.append(sentiment)\n",
    "\n",
    "\n",
    "            #4th column as output\n",
    "            tweet_message = row[3]\n",
    "\n",
    "            # @user\n",
    "            content = re.sub(r\"@[A-Za-z0-9_]+\", \" USERNAME \", tweet_message.lower())\n",
    "            # URL link\n",
    "            content = re.sub(r\"http\\S+\", \" URLLINK \", content)\n",
    "            #Remove repeater letter in a word e.g. heeeello to hello\n",
    "            content = re.sub(r\"([A-Za-z])\\1{2,}\", r\"\\1\", content)\n",
    "            #Replace all whitespace characters \n",
    "            content = re.sub(r\"\\s\", \" \", content)\n",
    "            # Replace EOS with END\n",
    "            content = re.sub(r\"(\\.|!|\\?) \", \" END \", content)\n",
    "            # Remove non-alphanumeric characters except spaces\n",
    "            content = re.sub(r\"[^A-Za-z0-9 ]\", \"\", content) \n",
    "            #Remove Pure Digits\n",
    "            content = re.sub(r\"\\bd+\\b\", \"\", content)\n",
    "            #Remove Single Letter words eg \"a\"\n",
    "            content = re.sub(r\"\\b[a-z0-9]\\b\", \"\", content)\n",
    "            #tokenize content\n",
    "            document =  nltk.word_tokenize(content)\n",
    "            clean_content_tokenized.append(document)\n",
    "            \n",
    "    return tweet_id_list,tweet_keyword_list, clean_content_tokenized, tweet_sentiment \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "improved-conjunction",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')\n",
    "\n",
    "def treebank_pos(word_tag):\n",
    "    #Return TREEBANK TAG Part-of-speech tag\n",
    "    if word_tag.startswith('V'): #verb\n",
    "        return 'v'\n",
    "    elif word_tag.startswith('N'): #noun \n",
    "        return 'n'\n",
    "    elif  word_tag.startswith('J'): #adjective\n",
    "        return 'a'\n",
    "    elif word_tag.startswith('R'): #adverb\n",
    "        return 'r'\n",
    "    else:\n",
    "        #set to noun if none is satistfied \n",
    "        return 'n'\n",
    "    \n",
    "def lemmatize_content (content):\n",
    "    '''\n",
    "    Transform words into their root from by using lemmisation with Treebank POS tagging\n",
    "    '''\n",
    "    content_lemmatized = []\n",
    "    content_pos=[]\n",
    "    content_clean=[]\n",
    "    \n",
    "    #lemmatize using POS tag\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    #POS tagging\n",
    "    for tweet in content:\n",
    "        #assign POS to each word \n",
    "        temp_pos = nltk.pos_tag(tweet)\n",
    "        content_pos.append(temp_pos)  \n",
    "        \n",
    "    for tweet in content_pos:\n",
    "        #Perform lemmatization on each POS word and return the lemmatized word as list\n",
    "        temp_lem = [ lemmatizer.lemmatize(word[0], pos=treebank_pos(word[1])) for word in tweet]\n",
    "        #Remove stop words\n",
    "        temp_lem = [word for word in temp_lem if word not in stopwords] \n",
    "        content_lemmatized.append(temp_lem) \n",
    "\n",
    "    for tweet in content_lemmatized:\n",
    "        #Concatenate content \n",
    "        temp_concat_content = \" \".join(tweet)\n",
    "        content_clean.append(temp_concat_content)\n",
    "        \n",
    "    return content_clean, content_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "operating-quality",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_keyword(keyword):\n",
    "    '''\n",
    "    Clean twitter keyword\n",
    "    Use porterstemmer to reduce keywords to their root form\n",
    "    '''\n",
    "    keyword_clean=[]\n",
    "    for key in keyword:\n",
    "        keyword_clean.append(re.sub(r\"%20\", \" \", key.lower()))\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "    stemmed_key =[]\n",
    "    for w in keyword_clean:\n",
    "        stemmed_key.append(ps.stem(w))\n",
    "    return stemmed_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "possible-poultry",
   "metadata": {},
   "source": [
    "### Preprocess Tweets\n",
    "Preprocess tweets by extracting tweet id, keyword, twitter content and sentiment from csv file\n",
    "Clean tweets and tokenise content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "nasty-track",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_tweets = {}\n",
    "preprocessed_tokenized_tweets = {}\n",
    "tweetgts = {} \n",
    "tweetids = {}\n",
    "tweetkey = {}\n",
    "\n",
    "tweetids['train'], tweetkey['train'], clean_content_tokenized, tweetgts['train']  = preprocess('train.csv', True)\n",
    "\n",
    "preprocessed_tweets['train'], preprocessed_tokenized_tweets['train'] = lemmatize_content(clean_content_tokenized)\n",
    "                        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "mysterious-scotland",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemm keyword so that we try and reduced keywords to their rootform\n",
    "tweetkey['train'] = preprocess_keyword(tweetkey['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hungry-traveler",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([4342, 3271], dtype=int64))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#output classes are almost balanced\n",
    "np.unique(tweetgts['train'], return_counts = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "measured-posting",
   "metadata": {},
   "source": [
    "###  Feature Function Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "british-cloud",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One Hot Encode Keyword\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "keyword_pd = pd.DataFrame(data = tweetkey['train'])\n",
    "encoded_key = enc.fit_transform(keyword_pd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "brutal-chain",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature processing using TFIDF\n",
    "# word level tf-idf\n",
    "def tfidf_word_level(content_train):\n",
    "    #Vectorize content by word with 2000 max features capping\n",
    "    tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\S+', max_features= 2000)\n",
    "    tfidf_vect.fit(content_train)\n",
    "    xtrain_tfidf =  tfidf_vect.transform(content_train)\n",
    "\n",
    "    xtrain_tfidf_np = xtrain_tfidf.todense()\n",
    "    xtrain_tfidf_np = np.array(xtrain_tfidf_np)\n",
    "\n",
    "    #return train set and vectorizer\n",
    "    return (xtrain_tfidf_np, tfidf_vect)\n",
    "\n",
    "# ngram level tf-idf\n",
    "def tfidf_ngram_level(content_train):\n",
    "    #Perform Bigram and Trigram Vectorization with max features of 5000\n",
    "    tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\S+', ngram_range=(2,3), max_features=5000) #non whitespace chars\n",
    "    tfidf_vect_ngram.fit(content_train)\n",
    "    xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(content_train)\n",
    "    \n",
    "    xtrain_tfidf_ngram_np = xtrain_tfidf_ngram.todense()\n",
    "    xtrain_tfidf_ngram_np= np.array(xtrain_tfidf_ngram_np)\n",
    "    \n",
    "    #return train set and vectorizer\n",
    "    return (xtrain_tfidf_ngram_np, tfidf_vect_ngram)\n",
    "\n",
    "\n",
    "#LSTM modelling\n",
    "def glove_LSTM_model(content_train, Y_train, no_tokens , embedding_dim):\n",
    "\n",
    "    # tokenize input content\n",
    "    tk = Tokenizer(num_words=no_tokens)\n",
    "    tk.fit_on_texts(content_train)\n",
    "    #convert text to sequence\n",
    "    content_seq = tk.texts_to_sequences(content_train)\n",
    "    \n",
    "    #Build Glove Dictionary\n",
    "    glove_file = 'glove.6B.100d.txt'\n",
    "    embedding_dict = {}\n",
    "    with open(glove_file,'r', encoding=\"utf8\") as glove:\n",
    "        for line in glove:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embedding_dict[word] = vector\n",
    "\n",
    "    #zero padding numpy array \n",
    "    embedding_matrix = np.zeros((no_tokens, embedding_dim))\n",
    "    \n",
    "    #retrieve 100 dimensional vector for each word\n",
    "    for word, index in tk.word_index.items():\n",
    "        if index < no_tokens:\n",
    "            vect = embedding_dict.get(word)\n",
    "            if vect is not None:\n",
    "                embedding_matrix[index] = vect\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    #get max train length\n",
    "    max_length = np.max([len(text.split()) for text in content_train])\n",
    "    #pad sequence\n",
    "    content_seq_trunc = pad_sequences(content_seq, maxlen=max_length)\n",
    "    \n",
    "    #encoding output\n",
    "    encoder = LabelEncoder()\n",
    "    y_train_encoded = encoder.fit_transform(Y_train)\n",
    "    y_train_categorical = to_categorical(y_train_encoded)\n",
    "    \n",
    "    earlystopping=EarlyStopping(monitor=\"val_loss\", patience=25, verbose=2, mode='auto', restore_best_weights=True)\n",
    "\n",
    "    #train model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(no_tokens, embedding_dim, weights = [embedding_matrix], trainable=False, input_length=max_length ))\n",
    "    model.add((LSTM(80, return_sequences=True)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add((LSTM(32)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=2, activation='softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer='rmsprop',metrics = ['accuracy'])\n",
    "    History = model.fit(content_seq_trunc,y_train_categorical,epochs = 100,batch_size=20,validation_split =0.2, callbacks=[earlystopping])\n",
    "    \n",
    "    return tk, max_length, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "irish-wales",
   "metadata": {},
   "source": [
    "### Dataset Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "answering-digest",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract features: word and ngram level tf-idf\n",
    "(xtrain_tfidf_np, tfidf_word_transformer) = tfidf_word_level(preprocessed_tweets['train'])        \n",
    "(xtrain_tfidf_ngram_np, tfidf_ngram_transformer) = tfidf_ngram_level(preprocessed_tweets['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "injured-dealing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate features\n",
    "xtrain_concat_features = np.concatenate((xtrain_tfidf_np, xtrain_tfidf_ngram_np,encoded_key.todense()), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-centre",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "superior-movie",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(xtrain_concat_features, tweetgts['train'], test_size=0.2,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "different-crazy",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(len(tweetgts['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "impossible-joshua",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_train, id_valid, y_train, y_valid = train_test_split(indices, tweetgts['train'], test_size=0.2,shuffle=True)\n",
    "X_train_first_classifiers = xtrain_concat_features[id_train ,  :]\n",
    "X_valid_first_classifiers = xtrain_concat_features[id_valid ,  :]\n",
    "\n",
    "\n",
    "preprocessed_tweets_np = np.array(preprocessed_tweets['train'])\n",
    "X_train_lstm_classifier = preprocessed_tweets_np[id_train]\n",
    "X_valid_lstm_classifier = preprocessed_tweets_np[id_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjusted-prompt",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "broke-network",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-frontier",
   "metadata": {},
   "source": [
    "### MultinomialNB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "original-specialist",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7977675640183848"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train sentiment Classifier 1 \n",
    "params = {'alpha': [0.001,0.01,0.1,1,10,50], 'fit_prior':[True,False]}\n",
    "model_nb_grid = GridSearchCV(naive_bayes.MultinomialNB(), params, n_jobs=-1, verbose=2, cv=3)\n",
    "model_nb_grid.fit(X_train_first_classifiers,y_train)\n",
    "Y_predicted_model_nb = model_nb_grid.predict(X_valid_first_classifiers)\n",
    "accuracy_score(Y_predicted_model_nb, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-saying",
   "metadata": {},
   "source": [
    "### LogisticRegression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "minor-geometry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7931713722915299"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train sentiment Classifier 2\n",
    "params = {'C': [0.001,0.01,0.1,1,10,50]}\n",
    "model_me_grid = GridSearchCV(LogisticRegression(), params, n_jobs=-1, verbose=-1, cv=3)\n",
    "model_me_grid.fit(X_train_first_classifiers,y_train)\n",
    "Y_predicted_model_me = model_me_grid.predict(X_valid_first_classifiers)\n",
    "accuracy_score(Y_predicted_model_me, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-yukon",
   "metadata": {},
   "source": [
    "### LSTM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "chicken-ancient",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "244/244 [==============================] - 3s 14ms/step - loss: 0.4901 - accuracy: 0.7752 - val_loss: 0.4804 - val_accuracy: 0.7882\n",
      "Epoch 2/100\n",
      "244/244 [==============================] - 3s 12ms/step - loss: 0.4423 - accuracy: 0.8048 - val_loss: 0.4722 - val_accuracy: 0.7882\n",
      "Epoch 3/100\n",
      "244/244 [==============================] - 3s 12ms/step - loss: 0.4236 - accuracy: 0.8132 - val_loss: 0.5272 - val_accuracy: 0.7734\n",
      "Epoch 4/100\n",
      "244/244 [==============================] - 3s 11ms/step - loss: 0.4016 - accuracy: 0.8255 - val_loss: 0.4749 - val_accuracy: 0.7791\n",
      "Epoch 5/100\n",
      "244/244 [==============================] - 3s 12ms/step - loss: 0.3816 - accuracy: 0.8368 - val_loss: 0.5024 - val_accuracy: 0.7890\n",
      "Epoch 6/100\n",
      "244/244 [==============================] - 3s 12ms/step - loss: 0.3593 - accuracy: 0.8483 - val_loss: 0.4753 - val_accuracy: 0.7890\n",
      "Epoch 7/100\n",
      "244/244 [==============================] - 3s 12ms/step - loss: 0.3370 - accuracy: 0.8592 - val_loss: 0.5018 - val_accuracy: 0.7980\n",
      "Epoch 8/100\n",
      "244/244 [==============================] - 3s 11ms/step - loss: 0.3117 - accuracy: 0.8699 - val_loss: 0.5115 - val_accuracy: 0.7660\n",
      "Epoch 9/100\n",
      "244/244 [==============================] - 3s 12ms/step - loss: 0.2857 - accuracy: 0.8846 - val_loss: 0.5392 - val_accuracy: 0.7824\n",
      "Epoch 10/100\n",
      "244/244 [==============================] - 3s 12ms/step - loss: 0.2639 - accuracy: 0.8900 - val_loss: 0.5442 - val_accuracy: 0.7857\n",
      "Epoch 11/100\n",
      "244/244 [==============================] - 3s 12ms/step - loss: 0.2342 - accuracy: 0.9093 - val_loss: 0.6493 - val_accuracy: 0.7923\n",
      "Epoch 12/100\n",
      "244/244 [==============================] - 3s 12ms/step - loss: 0.2138 - accuracy: 0.9103 - val_loss: 0.7252 - val_accuracy: 0.7701\n",
      "Epoch 13/100\n",
      "244/244 [==============================] - 3s 13ms/step - loss: 0.1907 - accuracy: 0.9210 - val_loss: 0.7636 - val_accuracy: 0.7677\n",
      "Epoch 14/100\n",
      "244/244 [==============================] - 3s 13ms/step - loss: 0.1700 - accuracy: 0.9325 - val_loss: 0.7773 - val_accuracy: 0.7734\n",
      "Epoch 15/100\n",
      "244/244 [==============================] - 3s 12ms/step - loss: 0.1590 - accuracy: 0.9331 - val_loss: 0.8442 - val_accuracy: 0.7709\n",
      "Epoch 16/100\n",
      "244/244 [==============================] - 3s 12ms/step - loss: 0.1376 - accuracy: 0.9444 - val_loss: 0.8760 - val_accuracy: 0.7611\n",
      "Epoch 17/100\n",
      "244/244 [==============================] - 3s 12ms/step - loss: 0.1240 - accuracy: 0.9491 - val_loss: 0.9861 - val_accuracy: 0.7701\n",
      "Epoch 18/100\n",
      "244/244 [==============================] - 3s 12ms/step - loss: 0.1164 - accuracy: 0.9557 - val_loss: 0.9803 - val_accuracy: 0.7578\n",
      "Epoch 19/100\n",
      "244/244 [==============================] - 3s 12ms/step - loss: 0.1054 - accuracy: 0.9563 - val_loss: 1.0662 - val_accuracy: 0.7611\n",
      "Epoch 20/100\n",
      "244/244 [==============================] - 3s 12ms/step - loss: 0.1011 - accuracy: 0.9600 - val_loss: 1.1336 - val_accuracy: 0.7668\n",
      "Epoch 21/100\n",
      "244/244 [==============================] - 3s 12ms/step - loss: 0.0852 - accuracy: 0.9682 - val_loss: 1.2685 - val_accuracy: 0.7767\n",
      "Epoch 22/100\n",
      "244/244 [==============================] - 3s 12ms/step - loss: 0.0839 - accuracy: 0.9641 - val_loss: 1.2495 - val_accuracy: 0.7668\n",
      "Epoch 23/100\n",
      "244/244 [==============================] - 3s 12ms/step - loss: 0.0794 - accuracy: 0.9667 - val_loss: 1.3933 - val_accuracy: 0.7553\n",
      "Epoch 24/100\n",
      "244/244 [==============================] - 3s 12ms/step - loss: 0.0764 - accuracy: 0.9682 - val_loss: 1.3852 - val_accuracy: 0.7504\n",
      "Epoch 25/100\n",
      "244/244 [==============================] - 3s 12ms/step - loss: 0.0739 - accuracy: 0.9692 - val_loss: 1.4091 - val_accuracy: 0.7447\n",
      "Epoch 26/100\n",
      "244/244 [==============================] - 3s 12ms/step - loss: 0.0692 - accuracy: 0.9713 - val_loss: 1.4192 - val_accuracy: 0.7734\n",
      "Epoch 27/100\n",
      "241/244 [============================>.] - ETA: 0s - loss: 0.0654 - accuracy: 0.9732Restoring model weights from the end of the best epoch.\n",
      "244/244 [==============================] - 3s 12ms/step - loss: 0.0653 - accuracy: 0.9733 - val_loss: 1.5831 - val_accuracy: 0.7348\n",
      "Epoch 00027: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7859487852921865"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train sentiment Classifier 3\n",
    "# Training an LSTM model with early stopping and Glove word embedding\n",
    "\n",
    "no_tokens = 5000  # Max no of tokens\n",
    "embedding_dim = 100  # Embedding dimensionality\n",
    "\n",
    "#fit model\n",
    "tokenizer_lstm, max_length_lstm, lstm_model = glove_LSTM_model(X_train_lstm_classifier.tolist(),y_train, no_tokens , embedding_dim)\n",
    "\n",
    "#tokenize and pad test tweets\n",
    "xvalid_seq = tokenizer_lstm.texts_to_sequences(X_valid_lstm_classifier.tolist())\n",
    "xvalid_seq_trunc = pad_sequences(xvalid_seq, maxlen=max_length_lstm)\n",
    "\n",
    "predict_lstm_categorical = lstm_model.predict(xvalid_seq_trunc)\n",
    "Y_predicted_lstm = np.argmax(predict_lstm_categorical, axis=1)\n",
    "accuracy_score(Y_predicted_lstm, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-masters",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "From the following results, it can be concluded that with a 20% validation split on the labelled training set, all models perform almost equally well. Logistic Regression turned out to be the top perfomer.\n",
    "\n",
    "The Multinomial Naive Bayes and the Logistic Regression models have the tweet encoded keyword, the word level and the ngram TF-IDF as input features. \n",
    "The LSTM model is based on the Glove 100D Embedding as input feature. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "endangered-democrat",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe for results storage\n",
    "results = pd.DataFrame(columns=['Accuracy','F1-Micro', 'F1-Macro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "digital-heater",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(act, pred):    \n",
    "    f1_micro= f1_score(act, pred,average='micro')\n",
    "    f1_macro = f1_score(act, pred,average='macro')\n",
    "    accuracy= accuracy_score(act, pred)\n",
    "    return accuracy, f1_micro, f1_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "strange-configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MultinomialNB Results\n",
    "accuracy, f1_micro, f1_macro = compute_metrics(Y_predicted_model_nb, y_valid)\n",
    "row = pd.Series({'Accuracy': accuracy, 'F1-Micro': f1_micro, 'F1-Macro': f1_macro},name='MultinomialNB')\n",
    "results = results.append(row)\n",
    "\n",
    "#LogisticRegression Results\n",
    "accuracy, f1_micro, f1_macro = compute_metrics(Y_predicted_model_me, y_valid)\n",
    "row = pd.Series({'Accuracy': accuracy, 'F1-Micro': f1_micro, 'F1-Macro': f1_macro},name='LogisticRegression')\n",
    "results = results.append(row)\n",
    "\n",
    "#LSTM Results\n",
    "accuracy, f1_micro, f1_macro = compute_metrics(Y_predicted_lstm, y_valid)\n",
    "row = pd.Series({'Accuracy': accuracy, 'F1-Micro': f1_micro, 'F1-Macro': f1_macro},name='LSTM')\n",
    "results = results.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "signed-malta",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-Micro</th>\n",
       "      <th>F1-Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MultinomialNB</th>\n",
       "      <td>0.797768</td>\n",
       "      <td>0.797768</td>\n",
       "      <td>0.787868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.793171</td>\n",
       "      <td>0.793171</td>\n",
       "      <td>0.784938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTM</th>\n",
       "      <td>0.785949</td>\n",
       "      <td>0.785949</td>\n",
       "      <td>0.768959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Accuracy  F1-Micro  F1-Macro\n",
       "MultinomialNB       0.797768  0.797768  0.787868\n",
       "LogisticRegression  0.793171  0.793171  0.784938\n",
       "LSTM                0.785949  0.785949  0.768959"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-office",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "official-lighter",
   "metadata": {},
   "source": [
    "### Test Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "changing-monroe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetids['test'], tweetkey['test'], clean_content_tokenized_test, tweetgts['test'],  = preprocess('test.csv', False)\n",
    "\n",
    "preprocessed_tweets['test'], preprocessed_tokenized_tweets['test'] = lemmatize_content(clean_content_tokenized_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "frequent-british",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemm keyword so that we try and reduced keywords to their rootform\n",
    "tweetkey['test'] = preprocess_keyword(tweetkey['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "biological-kernel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One Hot Encode Keyword\n",
    "keyword_pd_test = pd.DataFrame(data = tweetkey['test'])\n",
    "encoded_key_test = enc.fit_transform(keyword_pd_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "civil-orchestra",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract features: word and ngram level tf-idf\n",
    "(xtest_tfidf_np, tfidf_word_transformer) = tfidf_word_level(preprocessed_tweets['test'])        \n",
    "(xtest_tfidf_ngram_np, tfidf_ngram_transformer) = tfidf_ngram_level(preprocessed_tweets['test'])\n",
    "#concatenate features\n",
    "xtest_concat_features = np.concatenate((xtest_tfidf_np, xtest_tfidf_ngram_np,encoded_key_test.todense()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "grand-germany",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform Predictions\n",
    "\n",
    "#MultinomialNB\n",
    "Y_predicted_MultinomialNB_test = model_nb_grid.predict(xtest_concat_features)\n",
    "\n",
    "#LogisticRegression\n",
    "Y_predicted_LogisticRegression_test = model_me_grid.predict(xtest_concat_features)\n",
    "\n",
    "#LSTM\n",
    "preprocessed_tweets_np_test = np.array(preprocessed_tweets['test'])\n",
    "#tokenize and pad test tweets\n",
    "xvalid_seq = tokenizer_lstm.texts_to_sequences(preprocessed_tweets_np_test.tolist())\n",
    "xvalid_seq_trunc = pad_sequences(xvalid_seq, maxlen=max_length_lstm)\n",
    "\n",
    "predict_lstm_categorical = lstm_model.predict(xvalid_seq_trunc)\n",
    "Y_predicted_lstm_test = np.argmax(predict_lstm_categorical, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "literary-china",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export To Files\n",
    "np.savetxt('MultiNomianlNB_test.csv', Y_predicted_MultinomialNB_test, delimiter=',', fmt='%d')\n",
    "np.savetxt('LogisticRegression_test.csv', Y_predicted_LogisticRegression_test, delimiter=',', fmt='%d')\n",
    "np.savetxt('LSTM_test.csv', Y_predicted_lstm_test, delimiter=',', fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metallic-patent",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
